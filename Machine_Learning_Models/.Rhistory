ggplotly(lag15)
lag20<-
tsNYnew %>%
gg_lag(loansPerCapita,lags=20, geom='point') +
xlab(NULL) + ylab(NULL) +
ggtitle("Lag plot  with lag of 20 for the lending club NY data")
ggplotly(lag1)
lag25 <-
tsNYnew %>%
gg_lag(loansPerCapita,lags=25, geom='point') +
xlab(NULL) + ylab(NULL) +
ggtitle("Lag plot with lag of 25 for the lending club NY data")
ggplotly(lag20)
Ny_lPC <- tsNYnew %>%
mutate(`5-MA` = slide_dbl(loansPerCapita, mean, .size = 5, .align = "center"))
Ny_lPC %>%
autoplot(loansPerCapita)+
autolayer(Ny_lPC, `5-MA`, color = 'red')+
xlab("year") + ylab("GDP per cap")+
ggtitle(" loans per capita in NY" )+
guides(colour=guide_legend(title="series"))
tsNY2 <- tsLc %>%
filter(state == "NY") %>%
select(date, loansPerCapita)
plotNYNaive <-
tsNY2 %>%
model(SNAIVE(loansPerCapita), NAIVE(loansPerCapita~drift())) %>%
forecast(h = "5 years") %>%
autoplot(tsNY2) +
geom_line(linetype = 'dashed', colour = '#000000') +
xlab("Year(monthly data)") + ylab("Loans issues in NY") +
ggtitle("Forecast using naive and snaive model for NY lending club data")
plotNYNaive
Model1
tsNY2 <- tsLc %>%
filter(state == "NY") %>%
select(date, loansPerCapita)
plotNYNaive <-
tsNY2 %>%
model(SNAIVE(loansPerCapita), NAIVE(loansPerCapita~drift())) %>%
forecast(h = "5 years") %>%
autoplot(tsNY2) +
geom_line(linetype = 'dashed', colour = '#000000') +
xlab("Year(monthly data)") + ylab("Loans issues in NY") +
ggtitle("Forecast using naive and snaive model for NY lending club data")
plotNYNaive
#install.packages("tidyimpute")
#library(tidyimpute)
#tsNYnew2 <- drop_cols_all_na(tsNYnew)
#as.Date(as.yearmon(month))
set.seed(333)
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
sum(is.na(tsNY))
#install.packages("tidyimpute")
#library(tidyimpute)
#tsNYnew2 <- drop_cols_all_na(tsNYnew)
#as.Date(as.yearmon(month))
set.seed(333)
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
#features in tsNy
head(tsNY1)
NyTrain <- tsNY1 %>% filter(year(date) < 2014)
NyTest <- tsNY1 %>% filter(year(date) >= 2014)
#model 1 just with season and trend
modelseasontrend <- NyTrain %>%
model(TSLM(loansPerCapita ~ trend() +season() ))
report(modelseasontrend)
#model 2 with season, trend and other variables
model2 <-
NyTrain %>%
model(TSLM(loansPerCapita ~ trend() +season() +avgTerm +avgIntRate +avgGrade +avgAnnualInc ))
report(model2)
# avgTerm avgIntRate avgGrade avgEmpLength avgAnnualInc avgVerifStatus avgHomeOwner avgRevolBal countOfLoans
#install.packages("tidyimpute")
#library(tidyimpute)
#tsNYnew2 <- drop_cols_all_na(tsNYnew)
#as.Date(as.yearmon(month))
set.seed(333)
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
#features in tsNy
head(tsNY1)
NyTrain <- tsNY1 %>% filter(year(date) < 2014)
NyTest <- tsNY1 %>% filter(year(date) >= 2014)
#model 1 just with season and trend
modelseasontrend <- NyTrain %>%
model(TSLM(loansPerCapita ~ trend() +season() ))
report(modelseasontrend)
#model 2 with season, trend and other variables
model2 <-
NyTrain %>%
model(TSLM(loansPerCapita ~ trend() +season()  +avgIntRate +avgGrade +avgAnnualInc ))
report(model2)
# avgTerm avgIntRate avgGrade avgEmpLength avgAnnualInc avgVerifStatus avgHomeOwner avgRevolBal countOfLoans
#install.packages("tidyimpute")
#library(tidyimpute)
#tsNYnew2 <- drop_cols_all_na(tsNYnew)
#as.Date(as.yearmon(month))
set.seed(333)
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
#features in tsNy
#head(tsNY1)
NyTrain <- tsNY1 %>% filter(year(date) < 2014)
NyTest <- tsNY1 %>% filter(year(date) >= 2014)
#model 1 just with season and trend
modelseasontrend <- NyTrain %>%
model(TSLM(loansPerCapita ~ trend() +season() ))
report(modelseasontrend)
#model 2 with season, trend and other variables
model2 <-
NyTrain %>%
model(TSLM(loansPerCapita ~ trend() +season()  +avgIntRate +avgGrade +avgAnnualInc ))
report(model2)
# avgTerm avgIntRate avgGrade avgEmpLength avgAnnualInc avgVerifStatus avgHomeOwner avgRevolBal countOfLoans
#dataframe for NY records filtered below
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
fit1<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()))
report(fit1)
#dataframe for NY records filtered below
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
fit1<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()))
report(fit1)
fit2<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()
+ avgIntRate + avgGrade + avgAnnualInc))
report(fit2)
#dataframe for NY records filtered below
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
fit1<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()))
report(fit1)
fit2<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()
+ avgIntRate + avgGrade + avgAnnualInc))
report(fit2)
forecast1 <-
augment(fit1) %>%
ggplot(aes(x = date)) +
geom_line(aes(y = loansPerCapita, colour = "Data")) +
geom_line(aes(y = .fitted, colour = "Fitted")) +
xlab("Year") + ylab("Loans per capita for NY data") +
ggtitle("model 1- Loans Per Capita for lending club data of NY") +
scale_x_date(date_breaks = "years" , date_labels = "%y") +
guides(colour=guide_legend(title=NULL))
ggplotly(fitTsPlot)
#dataframe for NY records filtered below
tsNY1 <-tsLc %>%
filter(tsLc$state == 'NY')
#checking for na values
tsNY1 <- tsNY1[ , colSums(is.na(tsNY1)) == 0]
fit1<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()))
report(fit1)
fit2<- tsNY1 %>%
model(TSLM(loansPerCapita ~ trend() + season()
+ avgIntRate + avgGrade + avgAnnualInc))
report(fit2)
forecast1 <-
augment(fit1) %>%
ggplot(aes(x = date)) +
geom_line(aes(y = loansPerCapita, colour = "Data")) +
geom_line(aes(y = .fitted, colour = "Fitted")) +
xlab("Year") + ylab("Loans per capita for NY data") +
ggtitle("model 1- Loans Per Capita for lending club data of NY") +
scale_x_date(date_breaks = "years" , date_labels = "%y") +
guides(colour=guide_legend(title=NULL))
ggplotly(forecast1)
memory.limit()
memory.limit(size=900000000000)
memory.limit()
# Building the model
gbm_model <- gbm(formula = high_booking_rate~.,
data = df_train,
verbose = TRUE,shrinkage = 0.01,interaction.depth = 3,
distribution = "bernoulli",
n.trees = 4000, cv.folds = 2
)
library(xgboost)
library(readr)
library(stringr)
library(caret)
set.seed(12345)
library(class)
train_cleaned_final <- read_csv("GitHub/Airbnb-ML/Ash_Experiments/train_cleaned_final.csv")
train_cleaned_final <- read_csv("GitHub/Airbnb-ML/Ash_Experiments/train_cleaned4.csv")
## Randomly partition the data into 30% testing data and the remaining 70% data.
test_instn = sample(nrow(df), 0.3*nrow(df))
df_valid <- df[test_instn,]
## Save the rest of the data as the data that isn't testing
df_train <- df[-test_instn,]
names(df_valid)
train_cleaned_final <- read_csv("GitHub/Airbnb-ML/Ash_Experiments/train_cleaned4.csv")
df <- read_csv("GitHub/Airbnb-ML/Ash_Experiments/train_cleaned4.csv")
## Randomly partition the data into 30% testing data and the remaining 70% data.
test_instn = sample(nrow(df), 0.3*nrow(df))
df_valid <- df[test_instn,]
## Save the rest of the data as the data that isn't testing
df_train <- df[-test_instn,]
names(df_valid)
df_valid.y <- df_valid[,1]
df_valid.X<-df_valid[,2:53]
df_train.y <- df_train[,1]
df_train.X<-df_train[,2:53]
xgb <- xgboost(data = data.matrix(df_train.X),
label = df_train.y,
max_depth = 28,
nround=40,
eta = 0.75,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
#eval_metric = "merror",
objective = "binary:logistic",
#num_class = 12,
nthread = 3
)
length(df_train.X)
nrow(df_train.X)
nrow(df_train.y)
nrow(df_valid.y)
nrow(df_valid.X)
df <- read_csv("GitHub/Airbnb-ML/Ash_Experiments/train_cleaned4.csv")
View(df)
## Randomly partition the data into 30% testing data and the remaining 70% data.
test_instn = sample(nrow(df), 0.3*nrow(df))
df_valid <- df[test_instn,]
## Save the rest of the data as the data that isn't testing
df_train <- df[-test_instn,]
df_valid.y <- df_valid[,1]
df_valid.X<-df_valid[,2:67]
df_train.y <- df_train[,1]
df_train.X<-df_train[,2:67]
nrow(df_valid.X)
xgb <- xgboost(data = data.matrix(df_train.X),
label = df_train.y,
max_depth = 28,
nround=40,
eta = 0.75,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
#eval_metric = "merror",
objective = "binary:logistic",
#num_class = 12,
nthread = 3
)
nrow(df_train.y)
nrow(df_train.X)
xgb <- xgboost(data = data.matrix(df_train.X),
label = df_train.y,
max_depth = 28,
nround=40,
#eta = 0.75,
#subsample = 0.5,
#colsample_bytree = 0.5,
seed = 1,
#eval_metric = "merror",
objective = "binary:logistic",
#num_class = 12,
nthread = 3
)
setwd("~/GitHub/Airbnb-New/Machine_Learning_Models")
setwd(""~/GitHub/Airbnb-New/Machine_Learning_Models")
set.seed(12345)
df = read.csv("train_cleaned.csv")
df_competition = read.csv("test_cleaned.csv")
View(df_competition)
## Randomly partition the data into 30% testing data and the remaining 70% data.
test_instn = sample(nrow(df), 0.25*nrow(df))
df_test <- df[test_instn,]
## Save the rest of the data as the data that isn't testing
df_rest <- df[-test_instn,]
## e. Randomly partition the remaining data into 75% training data and 25% validation data.
valid_instn = sample(nrow(df_rest), 0.3*nrow(df_rest))
df_valid <- df_rest[valid_instn,]
df_train <- df_rest[-valid_instn,]
#View(df_competition)
model_log <- glm(high_booking_rate~accommodates+ amenities_count+
bathrooms+ Real_Bed+ bedrooms+ beds+ price+
cancellation_policy+require_guest_profile_picture+
guests_included+ host_about+host_has_profile_pic+
host_identity_verified+cleaning_fee+
host_is_superhost+
availability_60+availability_90+availability_30+
host_response_rate+ host_response_time+
instant_bookable+ is_business_travel_ready+
is_location_exact+ long_stay+
propertyApartment+ propertyCommon_house+
propertySide_house+ propertyHotel+ propertySpecial+
require_guest_phone_verification+
requires_license+
roomPrivate.room+
roomShared.room+ security_deposit+ flexible
, data = df_train,family="binomial")
summary(model_log)
#accommodates+ amenities_count+
#  availability_365+ availability_60+availability_90+availability_30+
#  bathrooms+ Real_Bed+ bedrooms+ beds+
#  cancellation_policy+ cleaning_fee+ extra_people+
#  first_review+ guests_included+ host_about+
#  host_has_profile_pic+ host_identity_verified+
#  host_is_superhost+ host_listings_count+
#  host_response_rate+ host_response_time+ experience+
#  instant_bookable+ is_business_travel_ready+
#  is_location_exact+ long_stay+ minimum_nights+
#  price+ propertyApartment+ propertyCommon_house+
#  propertySide_house+ propertyHotel+ propertySpecial+
#  require_guest_phone_verification+
#  require_guest_profile_picture+ requires_license+
#  roomEntire.home.apt+ roomPrivate.room+
#  roomShared.room+ security_deposit+ flexible
#lasso variable importance:
library('caret')
lambdaValues <- 10^seq(-3, 3, length = 100)
fitRidge <- train(as.factor(high_booking_rate)  ~
accommodates+ amenities_count+
availability_365+ availability_60+availability_90+availability_30+
bathrooms+ Real_Bed+ bedrooms+ beds+
cancellation_policy+ cleaning_fee+ extra_people+
first_review+ guests_included+ host_about+
host_has_profile_pic+ host_identity_verified+
host_is_superhost+ host_listings_count+
host_response_rate+ host_response_time+ experience+
instant_bookable+ is_business_travel_ready+
is_location_exact+ long_stay+ minimum_nights+
price+ propertyApartment+ propertyCommon_house+
propertySide_house+ propertyHotel+ propertySpecial+
require_guest_phone_verification+
require_guest_profile_picture+ requires_license+
roomEntire.home.apt+ roomPrivate.room+
roomShared.room+ security_deposit+ flexible
, family='binomial', data=df, method='glmnet', trControl=trainControl(method='cv', number=10), tuneGrid = expand.grid(alpha=0, lambda=lambdaValues))
library(dplyr)
varImp(fitRidge)$importance %>%
#rownames_to_column(var = "Variable") %>%
mutate(Importance = scales::percent(Overall/100)) %>%
arrange(desc(Overall)) %>%
as_tibble()
plot(varImp(fitRidge))
log_valid_probs <- predict(model_log, newdata = df_valid, type = "response")
library(ROCR)
pred <- prediction(log_valid_probs, df_valid$high_booking_rate)
tpr.perf = performance(pred, measure = "tpr")
tnr.perf = performance(pred, measure = "tnr")
acc.perf = performance(pred, measure = "acc")
plot(tpr.perf,ylim=c(0,1))
plot(tnr.perf, add=T,col='green')
plot(acc.perf,add=T,col = 'red')
#getting the accuracy against best cutoff; best stores the cutoff index
best = which.max(slot(acc.perf,"y.values")[[1]])
max.acc = slot(acc.perf,"y.values")[[1]][best]
max.cutoff = slot(acc.perf,"x.values")[[1]][best]
print(c(accuracy= max.acc, cutoff = max.cutoff))
confusion_matrix <- function(preds, actuals, cutoff){
classifications <- ifelse(preds>cutoff,1,0)
##careful with positives and negatives here!
confusion_matrix <- table(actuals,classifications)
}
#log_valid_preds = predict(model_log,newdata=df_valid,type="response")
log_matrix <- confusion_matrix(log_valid_probs, df_valid$high_booking_rate,0.4804465)
acc_log = (log_matrix[1] + log_matrix[4])/sum(log_matrix)
acc_log
#0.7856666
#0.7782773 -> lasso
#Trying to improve the model
log_matrix
#Now, fitting that to our test data subsetted from the training data
log_test_probs <- predict(model_log, newdata = df_test, type = "response")
log_matrix <- confusion_matrix(log_test_probs, df_test$high_booking_rate,0.4804465)
acc_log = (log_matrix[1] + log_matrix[4])/sum(log_matrix)
acc_log
#0.7848243
#0.7740075 -> lasso
#
log_test_probs = predict(model_log,newdata=df_competition,type="response")
log_test_probs
length(log_test_probs)
length(df_competition$high_booking_rate)
#pred <- prediction(log_test_probs, df_competition$high_booking_rate)
#log_matrix <- confusion_matrix(log_test_probs, df_competition$high_booking_rate,0.441866)
#library(ROCR)
#pred <- prediction(log_com_preds,df_test$high_booking_rate)
#acc_log = (log_matrix[1] + log_matrix[4])/sum(log_matrix)
#acc_log
#test$probs <- predict(movies.pruned, newdata=movies_test)[,2]
classpred <- ifelse(log_com_preds>.5,1,0)
length(classpred)
length(which(classpred==1))
#length(which(df$high_booking_rate==0))
#submission_probs <- dataframe(movies_test[,c(1,10)])
#submission_class <- movies_test[,c(1,11)]
#write.csv(submission_probs, "team0_probs.csv")
write.csv(classpred, "team7_class.csv")
setwd("~/GitHub/Airbnb-New/Machine_Learning_Models")
set.seed(12345)
df = read.csv("C:\Users\Aishwarya\Documents\GitHub\Airbnb-New\Data\train.csv")
df = read.csv(r"C:\Users\Aishwarya\Documents\GitHub\Airbnb-New\Data\train.csv")
df = read.csv("C:\\Users\\Aishwarya\\Documents\\GitHub\\Airbnb-New\\Data\\train.csv")
#df_competition = read.csv("test_cleaned.csv")
#View(df_competition)
## Randomly partition the data into 30% testing data and the remaining 70% data.
test_instn = sample(nrow(df), 0.25*nrow(df))
df_test <- df[test_instn,]
## Save the rest of the data as the data that isn't testing
df_rest <- df[-test_instn,]
## e. Randomly partition the remaining data into 75% training data and 25% validation data.
valid_instn = sample(nrow(df_rest), 0.3*nrow(df_rest))
df_valid <- df_rest[valid_instn,]
df_train <- df_rest[-valid_instn,]
model_log <- glm(high_booking_rate~.
, data = df_train,family="binomial")
summary(model_log)
log_valid_probs <- predict(model_log, newdata = df_valid, type = "response")
library(ROCR)
pred <- prediction(log_valid_probs, df_valid$high_booking_rate)
tpr.perf = performance(pred, measure = "tpr")
tnr.perf = performance(pred, measure = "tnr")
acc.perf = performance(pred, measure = "acc")
plot(tpr.perf,ylim=c(0,1))
plot(tnr.perf, add=T,col='green')
plot(acc.perf,add=T,col = 'red')
#getting the accuracy against best cutoff; best stores the cutoff index
best = which.max(slot(acc.perf,"y.values")[[1]])
max.acc = slot(acc.perf,"y.values")[[1]][best]
max.cutoff = slot(acc.perf,"x.values")[[1]][best]
print(c(accuracy= max.acc, cutoff = max.cutoff))
confusion_matrix <- function(preds, actuals, cutoff){
classifications <- ifelse(preds>cutoff,1,0)
##careful with positives and negatives here!
confusion_matrix <- table(actuals,classifications)
}
#log_valid_preds = predict(model_log,newdata=df_valid,type="response")
log_matrix <- confusion_matrix(log_valid_probs, df_valid$high_booking_rate,0.4804465)
#log_valid_preds = predict(model_log,newdata=df_valid,type="response")
log_matrix <- confusion_matrix(log_valid_probs, df_valid$high_booking_rate,0.4648672)
acc_log = (log_matrix[1] + log_matrix[4])/sum(log_matrix)
acc_log
#lasso variable importance:
library('caret')
lambdaValues <- 10^seq(-3, 3, length = 100)
fitRidge <- train(as.factor(high_booking_rate)  ~ .
, family='binomial', data=df, method='glmnet', trControl=trainControl(method='cv', number=10), tuneGrid = expand.grid(alpha=0, lambda=lambdaValues))
library(dplyr)
varImp(fitRidge)$importance %>%
#rownames_to_column(var = "Variable") %>%
mutate(Importance = scales::percent(Overall/100)) %>%
arrange(desc(Overall)) %>%
as_tibble()
plot(varImp(fitRidge))
#Now, fitting that to our test data subsetted from the training data
log_test_probs <- predict(model_log, newdata = df_test, type = "response")
log_matrix <- confusion_matrix(log_test_probs, df_test$high_booking_rate,0.4648672)
acc_log = (log_matrix[1] + log_matrix[4])/sum(log_matrix)
acc_log
## Randomly partition the data into 30% validation data and the remaining 70% data.
test_instn = sample(nrow(df), 0.3*nrow(df))
df_valid <- df[test_instn,]
## Save the rest of the data as the data that isn't validation
df_train <- df[-test_instn,]
#Check for any NAs
which(is.na(df_train))
df_train$high_booking_rate <- as.factor(df_train$high_booking_rate)
df_valid$high_booking_rate <- as.factor(df_valid$high_booking_rate)
#names(df_train)
#View(df)
#,"city_centrality","neighbourhood_restaurant",
#"maximum_nights","num_listings"
drop <-c("X1","X1_1")
df_valid <- df_valid[,-which(names(df_valid) %in% drop)]
df_train <- df_train[,-which(names(df_train) %in% drop)]
sum(is.na(df_train))
#---------------------------------------------------
#Hyper parameter tuning
res <- tuneRF(x = subset(df_train,select = - high_booking_rate),
y = df_train$high_booking_rate,
ntreeTry = 500)
#Importing the required libraries
library(randomForest)
require(caTools)
library(readr)
library(ROCR)
#best model, with tuned hyperparameters
rf<- randomForest(high_booking_rate~.,data = (df_train),
n_estimators = 50, min_sample_leaf = 80, mtry = 45,
na.rm = TRUE)
#---------------------------------------------------
#Hyper parameter tuning
res <- tuneRF(x = subset(df_train,select = - high_booking_rate),
y = df_train$high_booking_rate,
ntreeTry = 500)
